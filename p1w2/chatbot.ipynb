{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "hugging_face_token = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models, also known as efficient or lightweight language models, have gained significant attention in recent years due to their importance in various applications. Here are some key reasons why fast language models are crucial:\n",
      "\n",
      "1. **Scalability**: Fast language models can process large amounts of data and generate text at a much faster rate than traditional language models, making them ideal for use cases where speed and scalability are essential.\n",
      "2. **Real-time applications**: Fast language models can be used in real-time applications such as:\n",
      "\t* Chatbots: Immediate responses are critical in chatbots, and fast language models can provide quick and accurate responses.\n",
      "\t* Speech recognition: Real-time speech recognition relies on fast language models to transcribe speech quickly.\n",
      "\t* Natural Language Processing (NLP) pipelines: Fast language models can accelerate the processing of NLP pipelines, enabling faster analysis and decision-making.\n",
      "3. **Edge computing and IoT devices**: Fast language models can be deployed on edge computing devices and IoT devices, enabling on-device processing and reducing the need for cloud-based computing.\n",
      "4. **Conversational AI**: Fast language models are essential for conversational AI applications, such as voice assistants and smart speakers, which require rapid responses to user queries.\n",
      "5. **Low-latency applications**: Fast language models can reduce the latency in applications such as:\n",
      "\t* Text summarization\n",
      "\t* Language translation\n",
      "\t* Sentiment analysis\n",
      "\t* Question answering\n",
      "6. **Cost-effective**: Fast language models require less computational resources, which reduces the cost of training and deploying them.\n",
      "7. **Improved user experience**: Fast language models can provide a better user experience by:\n",
      "\t* Enabling instant responses to user queries\n",
      "\t* Reducing the time it takes to process and respond to user input\n",
      "\t* Providing more personalized and contextual responses\n",
      "8. **Healthcare and emergency response**: Fast language models can be used to quickly analyze medical texts, patient reports, and emergency messages, enabling healthcare professionals to respond rapidly to critical situations.\n",
      "9. **Cybersecurity**: Fast language models can be used to detect and respond to cyber threats in real-time, enhancing the effectiveness of cybersecurity measures.\n",
      "10. **Advancements in AI research**: Fast language models can accelerate research in AI by providing a faster and more efficient way to experiment with new ideas, train models, and analyze data.\n",
      "\n",
      "In summary, fast language models are essential for various applications that require rapid processing, scalability, and cost-effectiveness. They can improve user experience, enable faster decision-making, and accelerate advancements in AI research.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are gaining significant importance in various fields due to their ability to process and generate text at an impressive speed. Here's a breakdown of their key benefits and why they matter:\n",
      "\n",
      "**1. Real-Time Applications:**\n",
      "\n",
      "* **Chatbots & Conversational AI:** Fast response times are crucial for creating engaging and natural-sounding chatbots. Users expect immediate replies, and slow models can lead to frustrating experiences.\n",
      "* **Live Transcription & Translation:**  \n",
      "\n",
      "Real-time captioning for videos or translating spoken language on the fly relies heavily on the speed of language models. \n",
      "\n",
      "* **Interactive Systems:**  Applications like code completion, predictive text, and interactive storytelling benefit from the instantaneous feedback provided by fast models.\n",
      "\n",
      "**2. Efficiency and Scalability:**\n",
      "\n",
      "* **Reduced Latency:**  Faster processing reduces the time it takes to complete tasks, leading to more efficient workflows.\n",
      "* **Lower Computational Costs:** While some fast models might sacrifice accuracy for speed, they often require less computational power compared to larger, slower models. This makes them more accessible and scalable for deployment on various devices.\n",
      "\n",
      "**3. Enhanced User Experience:**\n",
      "\n",
      "* **Smoother Interactions:** The responsiveness of fast language models creates a more seamless and enjoyable user experience in applications like chatbots and interactive games.\n",
      "* **Increased Productivity:**  Faster task completion, such as real-time translation or summarization, can significantly boost productivity in professional settings.\n",
      "\n",
      "**4. Research and Development:**\n",
      "\n",
      "* **Exploring New Architectures:**  The focus on speed drives innovation in model architectures and training methods, leading to advancements in the field of AI.\n",
      "* **Benchmarking and Evaluation:**\n",
      "\n",
      "Fast models provide a valuable tool for benchmarking and comparing the performance of different language models across various tasks.\n",
      "\n",
      "\n",
      "**However, it's important to note that:**\n",
      "\n",
      "* **Accuracy vs. Speed:** There is often a trade-off between speed and accuracy.  While fast models have made significant strides, larger, slower models may still outperform them on tasks requiring high precision.\n",
      "* **Contextual Understanding:**  Fast models might struggle with complex tasks that require deep understanding of context and nuanced language.\n",
      "\n",
      "The future of language models likely lies in finding a balance between speed and accuracy, allowing for the development of increasingly sophisticated and versatile AI applications.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gemma2-9b-it\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are important for several reasons:\n",
      "\n",
      "1. Improved user experience: Fast language models can quickly generate responses to user queries, which leads to a more seamless and responsive user experience. This is especially important in real-time applications such as chatbots, virtual assistants, and speech recognition systems.\n",
      "2. Cost-effective: Fast language models can process large volumes of data quickly, which can reduce the computational resources required to train and deploy them. This can result in significant cost savings for organizations that rely on language models for their products and services.\n",
      "3. Scalability: Fast language models can handle larger datasets and more complex tasks than slower models. This makes them more scalable and adaptable to a wide range of applications, from social media monitoring to content generation.\n",
      "4. Real-time analytics: Fast language models can analyze data in real-time, which can provide valuable insights and feedback for businesses and organizations. For example, a fast language model can analyze customer feedback and identify trends and patterns in real-time, allowing businesses to respond quickly to emerging issues.\n",
      "5. Accessibility: Fast language models can make natural language processing (NLP) technologies more accessible to a wider range of users and devices. This can help democratize access to advanced NLP capabilities and enable more people to build innovative products and services.\n",
      "\n",
      "Overall, fast language models are an essential component of modern NLP systems, providing improved user experiences, cost savings, scalability, real-time analytics, and accessibility.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/KdG/dai6/DataAI6-KdG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris!"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    token=hugging_face_token,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. It is not only the most populous city in France, but it is also known for its history, culture, and architecture. Among these are iconic landmarks such as the Eiffel Tower, the Louvre Museum, which is the worldâ€™s largest art museum, and Notre-Dame Cathedral, known for its French Gothic architecture. Paris is also an important center for fashion, design, and gastronomy, adding to its reputation as a leading global city."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    token=hugging_face_token,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    token=hugging_face_token,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
