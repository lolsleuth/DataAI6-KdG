{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "load_dotenv()\n",
    "hugging_face_token = os.getenv(\"HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models have gained significant importance in recent years due to their ability to process and analyze large amounts of text data quickly and efficiently. Here are some reasons why fast language models are important:\n",
      "\n",
      "1. **Handling Big Data**: With the proliferation of digital data, large amounts of text data are being generated every day. Fast language models can handle this big data by processing it quickly and making it possible to analyze and gain insights from it.\n",
      "2. **Real-time Processing**: Fast language models enable real-time processing of text data, which is crucial in applications such as chatbots, customer service platforms, and social media monitoring. They can quickly respond to user queries and provide accurate information.\n",
      "3. **Improved Efficiency**: Traditional language models can be computationally expensive and time-consuming. Fast language models are designed to reduce processing time, making them more efficient and allowing for faster development and deployment of NLP applications.\n",
      "4. **Scalability**: Fast language models can be scaled up to handle large volumes of text data, making them suitable for applications that require processing massive amounts of data, such as sentiment analysis and entity recognition.\n",
      "5. **Accurate Results**: Fast language models can provide accurate results even with complex language patterns and nuanced semantics, which is essential for applications such as machine translation, question answering, and text summarization.\n",
      "6. **Low Latency**: Fast language models can provide low latency responses, which is critical in applications such as voice assistants, search engines, and recommendation systems.\n",
      "7. **Increased User Engagement**: Fast language models can lead to increased user engagement by providing quicker responses and more accurate results, resulting in improved customer satisfaction and loyalty.\n",
      "8. **Advancements in NLP**: The development of fast language models has driven advancements in natural language processing (NLP) and has enabled the creation of more sophisticated NLP applications, such as language translation, sentiment analysis, and text summarization.\n",
      "9. **Business and Economic Benefits**: Fast language models can provide significant business and economic benefits, such as improved customer service, increased user engagement, and enhanced decision-making capabilities, which can lead to increased revenue and competitive advantage.\n",
      "10. **Future-Proofing**: Fast language models can help future-proof NLP applications by enabling them to keep pace with the rapidly evolving nature of language, ensuring that they remain accurate and effective over time.\n",
      "\n",
      "In summary, fast language models are important because they enable efficient, accurate, and scalable natural language processing, which is essential for a wide range of applications, from customer service and chatbots to search engines and recommendation systems.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are becoming increasingly important for several reasons:\n",
      "\n",
      "**1. Responsiveness and Interactivity:** \n",
      "Fast response times are crucial for creating engaging and seamless user experiences.  In applications like chatbots, real-time translation, and interactive storytelling, delays can break the flow and frustrate users.\n",
      "\n",
      "**2. Efficiency and Scalability:** \n",
      "Fast models require less computational resources, making them more efficient to run. This is particularly important for deploying models on resource-constrained devices like smartphones or in large-scale applications serving millions of users.\n",
      "\n",
      "**3. Real-Time Applications:** \n",
      "Many emerging applications, such as voice assistants, real-time transcription, and automated content creation, rely on the ability to process language quickly. Fast models are essential for enabling these applications to function in real-time.\n",
      "\n",
      "**4. Personalization and Customization:**\n",
      "Fast models can be adapted to specific domains or user preferences more efficiently. This allows for personalized experiences, such as customized writing styles, tailored recommendations, or specialized knowledge bases.\n",
      "\n",
      "**5. Research and Development:**\n",
      "Faster training and inference times accelerate research in natural language processing. This leads to quicker experimentation, iteration, and the development of more advanced language models.\n",
      "\n",
      "**6. Accessibility:**\n",
      "Faster models can be deployed on less powerful hardware, making language technology more accessible to a wider range of users and organizations with limited resources.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "Developing fast language models presents several challenges:\n",
      "\n",
      "* **Trade-offs:** Achieving speed often involves sacrificing some accuracy or complexity. Finding the right balance is crucial.\n",
      "* **Data Efficiency:** \n",
      "Fast models may require less training data, but they still need sufficient data to learn effectively.\n",
      "* **Hardware Constraints:** While progress is being made, running very large language models in real-time can still be demanding on hardware resources.\n",
      "\n",
      "**Ongoing Research:**\n",
      "\n",
      "Research is constantly pushing the boundaries of speed in language models. This includes:\n",
      "\n",
      "* **Model Architecture Design:** Developing new model architectures that are more efficient and faster.\n",
      "* **Quantization and Pruning:** Reducing the precision of model weights and removing unnecessary connections to make them smaller and faster.\n",
      "* **Hardware Acceleration:** Utilizing specialized hardware like GPUs and TPUs to accelerate model training and inference.\n",
      "\n",
      "\n",
      "\n",
      "The increasing importance of fast language models is driving innovation and shaping the future of how we interact with technology.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gemma2-9b-it\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are important for a variety of reasons. Here are a few:\n",
      "\n",
      "1. Real-time processing: Fast language models can process and generate text in real-time, making them suitable for applications such as speech recognition, machine translation, and chatbots. Users expect quick and responsive interactions, and slow models can lead to frustration and abandonment of the application.\n",
      "2. Scalability: Fast language models can be deployed on large-scale systems, allowing them to handle high volumes of requests and data. This is important for applications such as search engines, social media platforms, and customer service systems that need to process large amounts of text data quickly and accurately.\n",
      "3. Cost-effectiveness: Fast language models can process large amounts of data efficiently, reducing the computational resources required to train and deploy them. This can result in cost savings for businesses and organizations, particularly those that need to handle large volumes of text data.\n",
      "4. Improved user experience: Fast language models can provide a more seamless and natural user experience, as they can respond quickly and accurately to user input. This is especially important for applications such as voice assistants and conversational agents, where users expect natural and responsive interactions.\n",
      "5. Competitive advantage: Fast language models can provide a competitive advantage for businesses and organizations that rely on text data processing and analysis. They can enable faster and more accurate processing of customer inquiries, improved search results, and better machine translation, giving businesses a leg up on their competitors.\n",
      "\n",
      "Overall, fast language models are essential for applications that require real-time processing, scalability, cost-effectiveness, improved user experience, and competitive advantage. They can enable businesses and organizations to process large volumes of text data quickly and accurately, providing valuable insights and improving user interactions.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris!"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    token=hugging_face_token,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## microsoft/Phi-3-mini-4k-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. It is not only the most populous city in France, but it is also known for its history, culture, and architecture. Among these are iconic landmarks such as the Eiffel Tower, the Louvre Museum, which is the world’s largest art museum, and Notre-Dame Cathedral, known for its French Gothic architecture. Paris is also an important center for fashion, design, and gastronomy, adding to its reputation as a leading global city."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    token=hugging_face_token,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    token=hugging_face_token,\n",
    ")\n",
    "\n",
    "for message in client.chat_completion(\n",
    "\tmessages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "\tmax_tokens=500,\n",
    "\tstream=True,\n",
    "):\n",
    "    print(message.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
